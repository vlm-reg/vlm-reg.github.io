<!DOCTYPE html>
<html>
<head>
  <link rel="shortcut icon" href="static/images/favicon.ico" type="image/x-icon">
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We reveal that state-of-the-art Vision-Language Models fail to generate pragmatically appropriate referring expressions. Unlike humans, these models often produce ambiguous or verbose descriptions that violate Gricean maxims of communication.">
  <meta property="og:title" content="VLMs Are Not Pragmatically Competent in Referring Expression Generation"/>
  <meta property="og:description" content="VLMs fail to refer like humans. Our study reveals widespread pragmatic issues in GPT-4o, LLaVA, and others, showing how their expressions often violate Gricean maxims."> 
  <meta property="og:url" content="https://vlm-reg.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/PixRefer.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="VLMs Fail at Pragmatic Referring Expressions">
  <meta name="twitter:description" content="VLMs fail to refer like humans. Our study reveals widespread pragmatic issues in GPT-4o, LLaVA, and others, showing how their expressions often violate Gricean maxims."> 
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/PixRefer.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-Language Models, Pragmatics, Referring Expression Generation">
  <meta name="author" content="Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai (SLED Lab, University of Michigan)">
  <meta name="robots" content="index, follow">

  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VLM-REG</title>
  <link rel="icon" type="image/x-icon" href="static/images/redketchup/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="static/images/redketchup/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/redketchup/favicon-16x16.png">
  <link rel="apple-touch-icon" href="static/images/redketchup/apple-touch-icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <!-- MathJax for LaTeX support -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <style>
    .carousel .item {
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
    }
    
    .carousel .item > img {
      margin: 0 auto;
      max-height: 450px;
      object-fit: contain;
    }
    
    .carousel .item .subtitle {
      margin-top: 15px;
    }
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div style="display: flex; align-items: center; justify-content: center; margin-bottom: 20px;">
              <h1 class="title is-1 publication-title">ðŸ’¬ Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation</h1>
            </div>
            <!-- <h4 class="title is-size-4 publication-title" style="color: red;">COLM 2025<span style="font-weight: normal; font-style: italic;"> (Under review)</span></h4>
            <h5 class="title is-size-5 publication-title" style="color: blueviolet;">GEM Workshop @ ACL 2025<span style="font-weight: normal; font-style: italic;"> (Under review)</span></h5>
            <h5 class="title is-size-5 publication-title" style="color: blueviolet;">CVinW Workshop @ CVPR 2025<span style="font-weight: normal; font-style: italic;"> (Under review)</span></h5> -->
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://mars-tin.github.io" target="_blank">Ziqiao Ma</a><sup>*,1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://jingding-ai.github.io" target="_blank">Jing Ding</a><sup>*,1</sup>,
              </span>
              <span class="author-block">
                <a href="https://xuejunzhang2002.github.io" target="_blank">Xuejun Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://ihzedoul.com" target="_blank">Dezhi Luo</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jiahe-ding-886814338/" target="_blank">Jiahe Ding</a><sup>1</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://sihanxu.github.io" target="_blank">Sihan Xu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/yuchen-huang-206756274/" target="_blank">Yuchen Huang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://roihn.github.io" target="_blank">Run Peng</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://web.eecs.umich.edu/~chaijy/" target="_blank">Joyce Chai</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>
                University of Michigan
                <sup>2</sup>
                GrowAI
              </span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                    <!-- PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/VLM_Pragmatics.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.16060" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                <!-- Supplementary PDF link
                <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Mars-tin/pixrefer.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Hugging Face link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Seed42Lab/RefOI" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ðŸ¤—
                  </span>
                  <span>RefOI Dataset</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/datasets/Seed42Lab/RefOI-TLHF" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  ðŸ¤—
                </span>
                <span>RefOI-TLHF Dataset</span>
              </a>
            </span>

              


            </div>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        TL;DR: We show significant pragmatic deficiencies in current VLMs when faced with referring expression generation compared to humans, as they violate Gricean maxims. 
      </h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/vlm_reg.mp4"
        type="video/mp4">
      </video>
      
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication. 
            However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. 
            In this work, we revisit REG from a pragmatic perspective, introducing a new dataset <code>RefOI</code> of 1.5k images annotated with both written and spoken referring expressions. 
            Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: <strong>(1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues.</strong> 
            We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. 
            Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- GIF carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">Data Collection Interfaces</h2>
      <!-- <p class="has-text-centered" style="margin-bottom: 2rem; font-size: 1rem; line-height: 1.6;">
        Our interfaces for data collection.
      </p> -->
    
      <div id="gif-carousel" class="carousel results-carousel">
        <div class="item">
          <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
            <!-- left gif -->
            <div style="text-align: center;">
              <img src="static/gifs/reg_text.gif" alt="reg text" style="width: 400px;">
              <div style="font-size: 0.9rem; margin-top: 10px;">Human Written</div>
            </div>
            
            <!-- right gif -->
            <div style="text-align: center;">
              <img src="static/gifs/reg_audio.gif" alt="reg speech" style="width: 400px;">
              <div style="font-size: 0.9rem; margin-top: 10px;">Human Spoken</div>
            </div>
          </div>
          <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
            The annotation interface. The user is required to enter text or provide a speech-to-text description of the object within the red box, ensuring that another observer can uniquely identify the object in the image.
            </div>
          </h3>
        </div>
    
        <div class="item">
          <img src="static/gifs/rel_merged.gif" alt="rel_merged">
          <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
              The human evaluation interface. The user is required to click on the corresponding object in the image based on the given description. The interface then displays the shortest distance between the clicked point and the nearest point on the mask of the target object, which will be 0 if the click is inside the mask. If the user cannot find any object matching the description or identifies multiple possible objects, they should click the corresponding button instead, and the distance will not be computed.
            </div>
          </h3>
        </div>
      
        <div class="item">
          <img src="static/gifs/minimal_words.gif" alt="Key Info Interface"/>
          <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
            The minimal words annotation interface. The user is required to select the minimal words that can uniquely identify the object. The input referring expressions in this part are the ones that are evaluated as successful in the human evaluation.
            </div>
          </h3>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End GIF carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">Qualitative Analyses</h2>
      <p class="has-text-centered" style="margin-bottom: 2rem; font-size: 1rem; line-height: 1.6;">
        We present qualitative examples illustrating the <strong>pragmatic differences</strong> between human and model-generated referring expressions.
        These examples highlight typical failure modes such as <em>overly verbose descriptions</em>, <em>irrelevant attribute mentions</em>, and <em>insufficient disambiguation</em>.
        The comparisons also reveal how humans tend to prefer <strong>concise, spatially grounded expressions</strong>, particularly in spoken settingsâ€”an aspect current VLMs often overlook.
      </p>
    
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/PixRefer-Compare.png" alt="MY ALT TEXT"/>
        <h3 class="subtitle has-text-centered">
          <div style="font-size: 1rem; margin-top: 10px;">
          Qualitative comparison of human and model referring expressions under Default and Brief prompts. Human expressions (especially in spoken form) tend to be concise and spatially grounded. In contrast, model outputs under Default prompts are often overly verbose, while Brief prompts reduce length but may omit pragmatically significant cues.
        </div>
        </h3>
      </div>
    
      <div class="item">
        <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
          
          <!-- Subfigure 1 -->
          <div style="text-align: center;">
            <img src="static/images/human_mixed_results.png" alt="Human" style="width: 250px; margin-top: 100px;">
            <div style="font-size: 0.9rem; margin-top: 10px;">Human</div>
          </div>
      
          <!-- Subfigure 2 -->
          <div style="text-align: center;">
            <img src="static/images/gpt4o_mixed.png" alt="GPT-4o" style="width: 250px; margin-top: 100px;">
            <div style="font-size: 0.9rem; margin-top: 10px;">GPT-4o</div>
          </div>
      
          <!-- Subfigure 3 -->
          <div style="text-align: center;">
            <img src="static/images/xcmp_mixed.png" alt="InternLM-XComposer" style="width: 250px; margin-top: 100px;">
            <div style="font-size: 0.9rem; margin-top: 10px;">XComposer</div>
          </div>
      
        </div>
      
        <!-- Optional group-level description -->
        <h3 class="subtitle has-text-centered" style="margin-top: 1rem;">
          <div style="font-size: 1rem; margin-top: 10px;">
          Word cloud comparison between human-produced expressions (aggregating written and spoken) and VLM-generated expressions (aggregating default and brief prompts) in <code>RefOI</code>. Semantically light words are filtered. Humans rely heavily on spatial cues, in line with prior findings, while VLMs favor visual features.
          </div>
        </h3>
      </div>
      
      <div class="item">
        <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
          
          <!-- Subfigure 1 -->
          <div style="text-align: center;">
            <img src="static/images/concise_length.png" alt="Image 1" style="width: 400px; margin-top: 100px;">
            <div style="font-size: 0.9rem; margin-top: 10px;">"Brief" Prompts</div>
          </div>
      
          <!-- Subfigure 2 -->
          <div style="text-align: center;">
            <img src="static/images/regular_length.png" alt="Image 2" style="width: 400px; margin-top: 100px;">
            <div style="font-size: 0.9rem; margin-top: 10px;">"Default" Prompts</div>
          </div>
     </div>
      
        <!-- Group-level description -->
        <h3 class="subtitle has-text-centered" style="margin-top: 1rem;">
          <div style="font-size: 1rem; margin-top: 10px;">
          Length distribution of referring expressions generated by the model using the two different prompts, including a comparison with human-written and spoken trials.
          </div>
        </h3>
    </div>
      

  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Results Table Section -->
<section class="section">
<div class="container is-max-desktop">
  <h2 class="title is-3 has-text-centered">Experiments</h2>
  <div class="content has-text-justified">
    <div style="overflow-x: auto;">
      <h3 class="title is-4">Main Results: Discrepancy Between Automatic Metrics and Human Judgement</h3>

      <!-- <table border="1" cellpadding="5" cellspacing="0" style="margin: 0 auto; font-size: 0.85em; max-width: 100%; border-collapse: collapse; border: 1px solid black; text-align: center;"> -->
        <table border="1" cellpadding="5" cellspacing="0" style="margin: 0 auto; font-size: 0.85em; max-width: 100%; border-collapse: collapse; border: 1.5px solid; text-align: center;">
          <tbody>
            <tr style="border-bottom: 1.5px solid;">
              <th>Model</th>
              <th>Instr.</th>
              <th>BLEU-1</th>
              <th>BLEU-4</th>
              <th>ROUGE-1</th>
              <th>ROUGE-L</th>
              <th>METEOR</th>
              <th>CIDEr</th>
              <th>SPICE</th>
              <th>BERT</th>
              <th>CLIP</th>
              <th>REC</th>
              <th>Human</th>
              <th>Irrel%</th>
            </tr>
        
            <!-- LLaVA-7B -->
            <tr>
              <td rowspan="2">LLaVA-7B</td>
              <td>Dft.</td><td>13.27</td><td>1.60</td><td>18.09</td><td>16.30</td><td>19.29</td><td>2.10</td><td>10.50</td><td>85.51</td><td>79.02</td><td>32.41</td><td>39.46</td><td>87.30</td>
            </tr>
            <tr>
              <td>Brf.</td><td>28.74</td><td>6.05</td><td><strong>36.46</strong></td><td>35.50</td><td>19.15</td><td>10.80</td><td>24.59</td><td>89.02</td><td>70.72</td><td>25.51</td><td>30.57</td><td>41.95</td>
            </tr>
        
            <!-- LLaVA-13B -->
            <tr>
              <td rowspan="2">LLaVA-13B</td>
              <td>Dft.</td><td>8.17</td><td>1.07</td><td>11.98</td><td>10.94</td><td>16.89</td><td>0.77</td><td>7.92</td><td>84.61</td><td>79.85</td><td>30.13</td><td>46.40</td><td>91.85</td>
            </tr>
            <tr>
              <td>Brf.</td><td>28.96</td><td>5.81</td><td>36.44</td><td><strong>35.64</strong></td><td>20.13</td><td>8.14</td><td>21.63</td><td>88.42</td><td>72.99</td><td>28.92</td><td>32.53</td><td>49.65</td>
            </tr>
        
            <!-- LLaVA-34B -->
            <tr>
              <td rowspan="2">LLaVA-34B</td>
              <td>Dft.</td><td>6.29</td><td>0.78</td><td>9.82</td><td>9.11</td><td>16.15</td><td>0.07</td><td>7.61</td><td>84.39</td><td>79.86</td><td>33.42</td><td>46.53</td><td>92.90</td>
            </tr>
            <tr>
              <td>Brf.</td><td>28.55</td><td>6.38</td><td>32.99</td><td>31.67</td><td>20.48</td><td>9.60</td><td>16.50</td><td>88.50</td><td>74.95</td><td>35.24</td><td>36.77</td><td>56.11</td>
            </tr>
        
            <!-- XComposer -->
            <tr>
              <td rowspan="2">XComposer</td>
              <td>Dft.</td><td>5.25</td><td>0.65</td><td>8.38</td><td>7.81</td><td>14.58</td><td>3.10</td><td>6.37</td><td>84.11</td><td>79.86</td><td>38.06</td><td>52.19</td><td>92.81</td>
            </tr>
            <tr>
              <td>Brf.</td><td>13.59</td><td>2.17</td><td>17.77</td><td>16.69</td><td>19.95</td><td>5.52</td><td>10.63</td><td>85.52</td><td>79.66</td><td>38.47</td><td>51.65</td><td>80.36</td>
            </tr>
        
            <!-- MiniCPM-V -->
            <tr>
              <td rowspan="2">MiniCPM-V</td>
              <td>Dft.</td><td>6.38</td><td>0.67</td><td>9.86</td><td>8.78</td><td>15.28</td><td>0.05</td><td>6.30</td><td>84.29</td><td>80.38</td><td>37.93</td><td>45.12</td><td>92.97</td>
            </tr>
            <tr>
              <td>Brf.</td><td>16.03</td><td>3.15</td><td>19.56</td><td>18.19</td><td>18.77</td><td>6.36</td><td>11.16</td><td>86.29</td><td>78.55</td><td>35.04</td><td>45.79</td><td>72.87</td>
            </tr>
        
            <!-- GLaMM -->
            <tr>
              <td rowspan="2">GLaMM</td>
              <td>Dft.</td><td>15.01</td><td>3.32</td><td>16.69</td><td>16.29</td><td>11.49</td><td>9.08</td><td>3.90</td><td>86.42</td><td>58.26</td><td>5.78</td><td>3.84</td><td>74.68</td>
            </tr>
            <tr>
              <td>Brf.</td><td>18.46</td><td>4.45</td><td>20.92</td><td>20.46</td><td>14.18</td><td>10.48</td><td>4.44</td><td>86.65</td><td>58.60</td><td>5.72</td><td>4.85</td><td>70.52</td>
            </tr>
        
            <!-- CogVLM -->
            <tr>
              <td rowspan="2">CogVLM</td>
              <td>Dft.</td><td>31.13</td><td><strong>8.70</strong></td><td>33.89</td><td>32.32</td><td>23.50</td><td><strong>41.62</strong></td><td>24.09</td><td>89.78</td><td>66.54</td><td>33.29</td><td>26.67</td><td><strong>26.39</strong></td>
            </tr>
            <tr>
              <td>Brf.</td><td><strong>31.39</strong></td><td>8.69</td><td>34.70</td><td>32.94</td><td><strong>24.87</strong></td><td>41.41</td><td><strong>24.74</strong></td><td><strong>90.00</strong></td><td>69.15</td><td>38.80</td><td>33.53</td><td>29.88</td>
            </tr>
        
            <!-- GPT-4o -->
            <tr>
              <td rowspan="2">GPT-4o</td>
              <td>Dft.</td><td>7.47</td><td>0.85</td><td>11.61</td><td>10.43</td><td>17.39</td><td>0.03</td><td>7.21</td><td>84.57</td><td><strong>80.81</strong></td><td><strong>41.29</strong></td><td><strong>59.80</strong></td><td>89.81</td>
            </tr>
            <tr>
              <td>Brf.</td><td>25.30</td><td>5.78</td><td>28.76</td><td>27.36</td><td>19.02</td><td>8.17</td><td>15.31</td><td>88.11</td><td>76.58</td><td>40.08</td><td>51.72</td><td>52.75</td>
            </tr>
        
            <!-- Human -->
            <tr style="border-top: 2px solid;">
              <td rowspan="2">Human</td>
              <td>Spk.</td><td>66.18</td><td>22.58</td><td>70.15</td><td>66.45</td><td>48.28</td><td>112.04</td><td>42.35</td><td>93.89</td><td>71.60</td><td>64.56</td><td>92.20</td><td>9.15</td>
            </tr>
            <tr>
              <td>Wrt.</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>70.43</td><td>63.69</td><td>89.29</td><td>7.29</td>
            </tr>
        
          </tbody>
        </table>
        
        
      <p style="margin-top: 15px; font-size: 0.9rem;">
        We compare model performance under different <strong>Instr.</strong> (Instruction) settings: <strong>Dft.</strong> (Default) prompt and <strong>Brf.</strong> (Brief) prompt. All model predictions are evaluated against Human <strong>Wrt.</strong> (Written) results as the reference texts. We also compute Human <strong>Spk.</strong> (Spoken) data in comparison with human-written data. <strong>Irrel%</strong> refers to the percentage of irrelevant words in the referring expression of the examples evaluated as successful.
      </p>

      <p style="margin-top: 20px; font-size: 1rem;">
        While human-generated expressions achieve over 90% accuracy in identifying objects, all VLMs fall short by a wide margin. GPT-4o performs best among the tested models, yet still lags behind humans. 
        Automatic metrics such as BLEU and CIDEr show poor correlation with human judgment, frequently ranking verbose models higher. 
        Even listener-based scores (REC) fail to consistently match human preferences, indicating that existing metrics do not capture pragmatic competence effectively.
      </p>
      
        </div>

    <div style="overflow-x: auto; margin-top: 30px;">
      <h3 class="title is-4" style="margin-top: 2rem;">Main Results Breakdown: Failures in Uniqueness and Identifiability</h3>

      <table border="1" cellpadding="5" cellspacing="0" style="margin: 0 auto; font-size: 0.75em; max-width: 100%; border-collapse: collapse; border: 1.5px solid black; text-align: center;">
        <tbody>
          <tr>
            <th rowspan="2">Model</th>
            <th rowspan="2">Instr.</th>
            <th colspan="3">Listener Compare</th>
            <th colspan="3">Error Breakdown</th>
            <th colspan="3">Class Breakdown</th>
            <th colspan="3">Class Co-occurrence</th>
          </tr>
          <tr style="border-bottom: 1.5px solid;">
            <th>Human</th>
            <th>REC</th>
            <th>Agree</th>
            <th>Wrong%</th>
            <th>Multi.%</th>
            <th>No-Mat%</th>
            <th>COCO</th>
            <th>No-COCO</th>
            <th>Î”<sub>Acc</sub></th>
            <th>Coocc.</th>
            <th>No-Coocc.</th>
            <th>Î”<sub>Acc</sub></th>
          </tr>
      
          <!-- LLaVA-7B -->
          <tr>
            <td rowspan="2">LLaVA-7B</td>
            <td>Dft.</td><td>39.46</td><td>32.41</td><td>65.84</td><td>14.62</td><td>40.40</td><td>5.52</td><td>41.26</td><td>37.65</td><td style="color: red;">-3.61</td><td>18.63</td><td>81.50</td><td style="color: red;">-62.87</td>
          </tr>
          <tr>
            <td>Brf.</td><td>30.57</td><td>25.51</td><td>71.62</td><td>10.23</td><td>52.26</td><td>6.94</td><td>31.18</td><td>29.96</td><td style="color: red;">-1.22</td><td>10.37</td><td>71.34</td><td style="color: red;">-60.97</td>
          </tr>
      
          <!-- LLaVA-13B -->
          <tr>
            <td rowspan="2">LLaVA-13B</td>
            <td>Dft.</td><td>46.40</td><td>30.13</td><td>65.10</td><td>26.26</td><td>26.20</td><td>1.14</td><td>45.70</td><td>47.10</td><td>1.40</td><td>28.80</td><td>81.91</td><td style="color: red;">-53.11</td>
          </tr>
          <tr>
            <td>Brf.</td><td>32.53</td><td>28.92</td><td>67.99</td><td>10.30</td><td>56.63</td><td>0.54</td><td>33.47</td><td>31.58</td><td style="color: red;">-1.89</td><td>10.67</td><td>76.63</td><td style="color: red;">-65.96</td>
          </tr>
      
          <!-- LLaVA-34B -->
          <tr>
            <td rowspan="2">LLaVA-34B</td>
            <td>Dft.</td><td>46.53</td><td>33.42</td><td>62.14</td><td>18.72</td><td>31.52</td><td>3.23</td><td>48.25</td><td>44.80</td><td style="color: red;">-3.45</td><td>29.41</td><td>81.10</td><td style="color: red;">-51.69</td>
          </tr>
          <tr>
            <td>Brf.</td><td>36.77</td><td>35.24</td><td>65.03</td><td>7.34</td><td>51.45</td><td>4.44</td><td>38.04</td><td>35.49</td><td style="color: red;">-2.55</td><td>15.11</td><td>80.59</td><td style="color: red;">-65.48</td>
          </tr>
      
          <!-- XComposer -->
          <tr>
            <td rowspan="2">XComposer</td>
            <td>Dft.</td><td>52.19</td><td>38.06</td><td>66.11</td><td>20.20</td><td>24.92</td><td>2.69</td><td>56.05</td><td>48.31</td><td style="color: red;">-7.74</td><td>37.56</td><td>81.70</td><td style="color: red;">-44.14</td>
          </tr>
          <tr>
            <td>Brf.</td><td>51.65</td><td>38.47</td><td>64.09</td><td>14.28</td><td>31.45</td><td>2.62</td><td>55.78</td><td>47.50</td><td style="color: red;">-8.28</td><td>35.55</td><td>84.15</td><td style="color: red;">-48.60</td>
          </tr>
      
          <!-- MiniCPM-V -->
          <tr>
            <td rowspan="2">MiniCPM-V</td>
            <td>Dft.</td><td>45.12</td><td>37.93</td><td>66.38</td><td>15.75</td><td>34.55</td><td>4.58</td><td>47.98</td><td>42.24</td><td style="color: red;">-5.74</td><td>26.49</td><td>82.72</td><td style="color: red;">-56.23</td>
          </tr>
          <tr>
            <td>Brf.</td><td>45.79</td><td>35.04</td><td>63.62</td><td>12.19</td><td>38.99</td><td>3.03</td><td>49.46</td><td>42.11</td><td style="color: red;">-7.35</td><td>26.99</td><td>83.74</td><td style="color: red;">-56.75</td>
          </tr>
      
          <!-- GLaMM -->
          <tr>
            <td rowspan="2">GLaMM</td>
            <td>Dft.</td><td>3.84</td><td>5.78</td><td>93.61</td><td>7.33</td><td>15.29</td><td>73.54</td><td>4.30</td><td>3.37</td><td style="color: red;">-0.93</td><td>1.31</td><td>8.94</td><td style="color: red;">-7.63</td>
          </tr>
          <tr>
            <td>Brf.</td><td>4.85</td><td>5.72</td><td>93.34</td><td>8.49</td><td>14.07</td><td>72.59</td><td>4.30</td><td>5.40</td><td>1.10</td><td>1.31</td><td>11.99</td><td style="color: red;">-10.68</td>
          </tr>
      
          <!-- CogVLM -->
          <tr>
            <td rowspan="2">CogVLM</td>
            <td>Dft.</td><td>26.67</td><td>33.29</td><td>73.30</td><td>2.89</td><td>47.34</td><td>23.10</td><td>27.96</td><td>25.37</td><td style="color: red;">-2.59</td><td>13.39</td><td>53.46</td><td style="color: red;">-40.07</td>
          </tr>
          <tr>
            <td>Brf.</td><td>33.53</td><td>38.80</td><td>68.53</td><td>2.96</td><td>52.53</td><td>10.98</td><td>34.81</td><td>32.25</td><td style="color: red;">-2.56</td><td>16.72</td><td>67.48</td><td style="color: red;">-50.76</td>
          </tr>
      
          <!-- GPT-4o -->
          <tr>
            <td rowspan="2">GPT-4o</td>
            <td>Dft.</td><td>59.80</td><td>41.29</td><td>62.00</td><td>11.98</td><td>24.04</td><td>4.18</td><td>63.31</td><td>56.28</td><td style="color: red;">-7.03</td><td>48.14</td><td>83.33</td><td style="color: red;">-35.19</td>
          </tr>
          <tr>
            <td>Brf.</td><td>51.72</td><td>40.08</td><td>63.01</td><td>10.97</td><td>31.52</td><td>5.79</td><td>54.84</td><td>48.58</td><td style="color: red;">-6.26</td><td>37.36</td><td>80.69</td><td style="color: red;">-43.33</td>
          </tr>
      
          <!-- Human -->
          <tr style="border-top: 2px solid;">
            <td rowspan="2">Human</td>
            <td>Spk.</td><td>92.20</td><td>64.56</td><td>64.96</td><td>6.93</td><td>0.74</td><td>0.13</td><td>92.07</td><td>92.58</td><td>0.51</td><td>91.74</td><td>93.50</td><td style="color: red;">-1.76</td>
          </tr>
          <tr>
            <td>Wrt.</td><td>89.29</td><td>63.69</td><td>63.69</td><td>7.68</td><td>2.36</td><td>0.67</td><td>89.52</td><td>89.07</td><td style="color: red;">-0.45</td><td>88.31</td><td>91.26</td><td style="color: red;">-2.95</td>
          </tr>
        </tbody>
      </table>
      
      <p style="margin-top: 15px; font-size: 0.9rem;">
        <strong>Listener Compare:</strong> The human evaluation accuracy with <strong>REC</strong> (the evaluation result from CogVLM-Grounding) and computes <strong>Agree</strong> (the agreement between the two listeners). <br>
        <strong>Error Breakdown:</strong> The percentages of three types of errors: <strong>Wrong</strong> refers to a failed guess, <strong>Multi.</strong> refers to multiple potential matches, and <strong>No-Mat</strong> refers to cases where no object can be located. <br>
        <strong>Class Breakdown:</strong> The accuracy of COCO-class objects with non-COCO-class objects. The metric <strong>Î”<sub>Acc</sub></strong> shows the accuracy drop between the two categories. <br>
        <strong>Class Co-occurrence:</strong> The accuracy of <strong>Coocc.</strong> images (images containing more than one object of the same class) with <strong>No-Coocc.</strong> images (images containing only one object of its class). <strong>Î”<sub>Acc</sub></strong> denotes the accuracy drop between these two categories.
      </p>

      <p style="margin-top: 20px; font-size: 1rem;">
        The breakdown reveals that many failures stem from referential ambiguity, particularly when multiple similar objects appear in the same scene.
        Models often fail to generate uniquely identifying expressions under such conditions.
        Performance also drops significantly for non-COCO classes, suggesting dataset bias and poor generalization beyond common benchmarks.
        All models struggle with images containing co-occurring objects, indicating limited ability to leverage distinctive features for disambiguation.
      </p>
      
      </div>
    </div>
  </div>
</section>
<!-- End Results Table Section -->



<!-- Further Analysis & Discussion -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Further Analyses and Discussions</h2>

    <!-- Misalignment Analysis -->
    <div class="content has-text-justified">
      <h3 class="title is-4">Misalignment to Human Pragmatic Preferences 
        <a href="https://huggingface.co/datasets/Seed42Lab/RefBlock" target="_blank" class="has-text-link" style="margin-left: 15px; font-size: 0.8em; text-decoration: none;">
          [Synthetic Data]
        </a>
      </h3>
      <p>
        While humans heavily rely on <strong>spatial cues</strong> in referring expressions, VLMs often favor <strong>combinations of visual attributes</strong> such as shape and color. This divergence reveals that VLMs may not follow human pragmatic preferences when multiple minimal descriptions are availableâ€”violating Gricean maxims of <em>Relation</em> and <em>Quantity</em>.
      </p>
      <p>
        To isolate this effect, we design a synthetic dataset where each referent can be uniquely described using one of four independent features: <strong>size</strong>, <strong>color</strong>, <strong>shape</strong>, or <strong>position</strong>. Human responses are collected and compared with VLM likelihoods.
      </p>
      <p>
        Results show that humans are highly sensitive to visual saliency and prefer the most contextually informative cue. VLMs, in contrast, display flatter preference distributions and less discriminative usage of featuresâ€”highlighting a lack of pragmatic grounding.
      </p>
    </div>

    <!-- Carousel for Synthetic Preference Visuals -->
    <div class="container is-max-desktop">
      <div id="analysis-carousel" class="carousel results-carousel">
        <!-- Figure 1 -->
        <div class="item" style="display: flex; flex-direction: column; justify-content: center; align-items: center;">
          <img src="static/images/PixRefer-Syn.png" alt="Syn Data Generation" style="margin-top: 200px;"/>
          <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
            Synthetic dataset. <br> Left: The gradient manipulation for each visual feature (size, color, position, shape), where the target object (red arrow) remains constant while the distractor varies along a single dimension. <br> Right: Example of a trial in which "left," "lighter," and "smaller" can all uniquely identify the referent. Human speakers predominantly choose the spatial descriptor, whereas the VLM prefers attribute-based expressions.
            </div>
          </h3>
        </div>

        <!-- Figure 2 -->
        <div class="item" style="display: flex; flex-direction: column; justify-content: center; align-items: center;">
          <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
            
            <!-- Subfigure 1 -->
            <div style="text-align: center;">
              <img src="static/images/light.jpg" alt="Human" style="width: 300px; margin-top: 50px;">
            </div>
        
            <!-- Subfigure 2 -->
            <div style="text-align: center;">
              <img src="static/images/square.jpg" alt="GPT-4o" style="width: 300px; margin-top: 50px;">
            </div>
        
            <!-- Subfigure 3 -->
            <div style="text-align: center;">
              <img src="static/images/small.jpg" alt="InternLM-XComposer" style="width: 300px;">
            </div>

            <!-- Subfigure 4 -->
            <div style="text-align: center;">
              <img src="static/images/left.jpg" alt="InternLM-XComposer" style="width: 300px;">
            </div>
        
          </div>
        
          <!-- Optional group-level description -->
          <h3 class="subtitle has-text-centered" style="margin-top: 1rem;">
            <div style="font-size: 1rem; margin-top: 10px;">
            Attribute selection as a function of feature salience. Across all four dimensions, humans readily attend to feature salience when selecting attributes for reference, whereas VLMs exhibit weaker sensitivity.
            </div>
          </h3>
        </div>

        <!-- Figure 3 - Two rows, each with 5 images -->
        <div class="item" style="display: flex; flex-direction: column; justify-content: center; align-items: center;">
          <!-- Top Row -->
          <div style="margin-bottom: 30px; width: 100%;">
            <div style="display: flex; justify-content: center; align-items: center; gap: 5px; flex-wrap: wrap;">
              <!-- Top Row - 5 Subfigures -->
              <div style="text-align: center;">
                <img src="static/images/heatmap/left_small_7b.jpg" alt="LLaVA-7B" style="width: 200px;">
                <div style="font-size: 0.9rem;">LLaVA-7B</div>
              </div>
              
              <div style="text-align: center;">
                <img src="static/images/heatmap/left_small_13b.jpg" alt="LLaVA-13B" style="width: 200px;">  
                <div style="font-size: 0.9rem;">LLaVA-13B</div>
              </div>
              
              <div style="text-align: center;">
                <img src="static/images/heatmap/left_small_34b.jpg" alt="LLaVA-34B" style="width: 200px;">
                <div style="font-size: 0.9rem;">LLaVA-34B</div>
              </div>
              
              <div style="text-align: center;">
                <img src="static/images/heatmap/left_small_human.jpg" alt="Human" style="width: 200px;">
                <div style="font-size: 0.9rem;">Human</div>
              </div>
              
              <div style="text-align: center;">
                <img src="static/images/heatmap/value_bar.jpg" alt="Legend" style="width: 46px; margin-bottom: 34pt;">
              </div>
            </div>
            
            <h3 class="subtitle has-text-centered" style="margin-top: 1rem;">
              <div style="font-size: 1rem; margin-top: 10px;">
              Heatmap showing the difference in normalized probability of choosing "left" over "small", calculated as \(\hat{p} = \hat{p}_{\textrm{small}} - \hat{p}_{\textrm{left}}\). Darker colors indicate a preference for using the spatial term "left" over the size term "small".
              </div>
            </h3>
          </div>
          
          <!-- Bottom Row -->
          <div style="width: 100%;">
            <div style="display: flex; justify-content: center; align-items: center; gap: 5px; flex-wrap: wrap;">
              <!-- Bottom Row - 5 Subfigures -->
              <div style="text-align: center;">
                <img src="static/images/heatmap/left_square_7b.jpg" alt="CogVLM" style="width: 200px;">
                <div style="font-size: 0.9rem;">LLaVA-7B</div>
              </div>
              
              <div style="text-align: center;">
                <img src="static/images/heatmap/left_square_13b.jpg" alt="MiniCPM" style="width: 200px;">
                <div style="font-size: 0.9rem;">LLaVA-13B</div>
              </div>
              
              <div style="text-align: center;">
                <img src="static/images/heatmap/left_square_34b.jpg" alt="GLaMM" style="width: 200px;">
                <div style="font-size: 0.9rem;">LLaVA-34B</div>
              </div>
              
              <div style="text-align: center;">
                <img src="static/images/heatmap/left_square_human.jpg" alt="GPT-4o" style="width: 200px;">
                <div style="font-size: 0.9rem;">Human</div>
              </div>
              
              <div style="text-align: center;">
                <img src="static/images/heatmap/value_bar.jpg" alt="Legend" style="width: 46px; margin-bottom: 34pt;">
              </div>
            </div>
            
            <h3 class="subtitle has-text-centered" style="margin-top: 1rem;">
              <div style="font-size: 1rem; margin-top: 10px;">
              Heatmap showing the difference in normalized probability of choosing "square" over "small", calculated as \(\hat{p} = \hat{p}_{\textrm{small}} - \hat{p}_{\textrm{square}}\). Darker colors indicate a preference for using the spatial term "square" over the size term "small".
              </div>
            </h3>
          </div>
        </div>

      </div>
    </div>

    <!-- Eval Metric Critique -->
    <div class="content has-text-justified">
      <h3 class="title is-4" style="margin-top: 100px;">Why Is Current Automatic Evaluation Unreliable?</h3>
      <p>
        Standard metrics such as BLEU, METEOR, and even model-based metrics like CLIPScore fail to capture key pragmatic aspects. For instance, overly brief yet sufficient expressions (e.g., "cookie") are penalized by BLEU, and paraphrased spatial constructions are unfairly punished by METEOR's fragmentation penalty.
      </p>
      <p>
        Model-based similarity metrics also blur pragmatic distinctions. For example, "largest cookie" and "cookie" may score similarly under CLIPScore despite large differences in informativeness. Listener-based metrics further compound the issue by reinforcing biases toward salient objects.
      </p>
      <p>
        These issues highlight the <strong>urgent need for pragmatically aware evaluation frameworks</strong> that reflect human-like judgments.
      </p>
      
      <!-- Add image -->
      <div style="text-align: center; margin: 30px 0;">
        <img src="static/images/PixRefer-Eval.png" alt="Evaluation Metrics Comparison" style="max-width: 90%; height: auto;">
        <div style="font-size: 1rem; margin-top: 10px;">
          A case study illustrating why automatic metrics, including heuristic measures and neural listener models, fail to accurately capture the pragmatic performance of REG.
        </div>
      </div>
    </div>

    <!-- Dataset Recommendations -->
    <div class="content has-text-justified">
      <h3 class="title is-4" style="margin-top: 100px;">Recommended Use of Our Dataset</h3>
      <p>
        The <code>RefOI</code> dataset is designed for fine-grained REG/REC analysis. It distinguishes between <strong>COCO and non-COCO classes</strong>, and between scenes with <strong>single vs. multiple distractors</strong> of the same class. 
      </p>
      <p>
        We encourage users to leverage these distinctions for deeper insights and invite community contributions to expand non-COCO annotations.
      </p>
      </div>
    </div>
  </section>
<!-- End Further Analysis & Discussion -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@misc{ma2025visionlanguagemodelspragmaticallycompetent,
  title={Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation}, 
  author={Ziqiao Ma and Jing Ding and Xuejun Zhang and Dezhi Luo and Jiahe Ding and Sihan Xu and Yuchen Huang and Run Peng and Joyce Chai},
  year={2025},
  eprint={2504.16060},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2504.16060}, 
}
        </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->